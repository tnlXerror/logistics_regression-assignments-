{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tbVTCt1HQM79"
      },
      "outputs": [],
      "source": [
        "1. What is Logistic Regression, and How Does It Differ from Linear Regression?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Logistic Regression is used when the dependent variable is binary (categorical) in nature. Unlike Linear Regression, which predicts continuous outcomes, Logistic Regression predicts the probability that an observation belongs to a particular class (e.g., yes/no, 1/0).\n"
      ],
      "metadata": {
        "id": "obpX_9zOQhIm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is the Mathematical Equation of Logistic Regression?"
      ],
      "metadata": {
        "id": "tXxMrT2wQknV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The equation for Logistic Regression is based on the Sigmoid function:\n",
        "P(y=1∣x)=11+e−(b0+b1x)P(y = 1|x) = \\frac{1}{1 + e^{-(b_0 + b_1x)}}P(y=1∣x)=1+e−(b0​+b1​x)1​\n",
        "Here:\n",
        "P(y=1∣x)P(y = 1|x)P(y=1∣x) represents the probability that the output is 1 (e.g., disease is present).\n",
        "\n",
        "\n",
        "b0+b1xb_0 + b_1xb0​+b1​x is the linear combination of predictors (e.g., age, blood pressure).\n",
        "\n",
        "\n",
        "The Sigmoid function squashes the result to a value between 0 and 1."
      ],
      "metadata": {
        "id": "1uiVDnJVQnp0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Why Do We Use the Sigmoid Function in Logistic Regression?"
      ],
      "metadata": {
        "id": "iKqDQJlGQqt1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analogy:\n",
        "Think of the Sigmoid function as a funnel. Anything you input (the output of the linear equation) gets funneled into a narrow range from 0 to 1, which is perfect for interpreting it as a probability.\n",
        "Reason:\n",
        "The Sigmoid function transforms any input (whether positive or negative) into a probability, which is crucial for binary classification. For example, if the probability is greater than 0.5, you predict class 1; if it’s less than 0.5, you predict class 0.\n"
      ],
      "metadata": {
        "id": "C52zV39tQsoN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What is the Cost Function of Logistic Regression?\n"
      ],
      "metadata": {
        "id": "MmCVi9XZQ0H0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imagine you are baking a pizza, and your goal is to make it taste as good as possible. The cost function is like the taste test—it measures how far your current pizza is from the ideal pizza. The lower the cost, the closer you are to the \"perfect\" pizza!\n",
        "Mathematical Equation:\n",
        "The Log-Loss or Binary Cross-Entropy cost function for Logistic Regression is:\n",
        "Cost=−1N∑[ylog⁡(p)+(1−y)log⁡(1−p)]\\text{Cost} = - \\frac{1}{N} \\sum \\left[ y \\log(p) + (1-y) \\log(1-p) \\right]Cost=−N1​∑[ylog(p)+(1−y)log(1−p)]\n",
        "Where:\n",
        "yyy is the actual outcome (0 or 1),\n",
        "\n",
        "\n",
        "ppp is the predicted probability that y=1y = 1y=1.\n",
        "\n",
        "\n",
        "Explanation:\n",
        "This function penalizes wrong predictions:\n",
        "If the true outcome is 1, and the model predicts a low probability (near 0), the cost will be high.\n",
        "\n",
        "\n",
        "If the true outcome is 0, and the model predicts a high probability (near 1), the cost will also be high.\n",
        "\n",
        "\n",
        "The goal is to minimize this cost function during training"
      ],
      "metadata": {
        "id": "_DvBUtxAQ62F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What is Regularization in Logistic Regression? Why Is It Needed?"
      ],
      "metadata": {
        "id": "8W29InGLRAYk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Think of regularization as a diet plan. Without it, your model might \"overeat\" and become overfit, meaning it works great for training data but poorly for new data. Regularization ensures your model stays \"healthy\" by preventing it from becoming too complex (overfitting).\n",
        "Reason:\n",
        "Regularization adds a penalty term to the cost function to control the size of the coefficients. This penalty prevents the model from fitting noise and ensures that it generalizes well to new data.\n"
      ],
      "metadata": {
        "id": "zMq5IDbTRCHl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Difference Between Lasso, Ridge, and Elastic Net Regression\n"
      ],
      "metadata": {
        "id": "x18rcnpLREnU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lasso (L1 Regularization): Shrinks some coefficients to zero, effectively performing feature selection.\n",
        "\n",
        "\n",
        "Ridge (L2 Regularization): Shrinks coefficients but does not eliminate them, helping to reduce overfitting.\n",
        "\n",
        "\n",
        "Elastic Net: Combines Lasso and Ridge penalties, balancing between feature selection and coefficient shrinkage.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_AhNDZYaRGhb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. When Should We Use Elastic Net?\n"
      ],
      "metadata": {
        "id": "ccOxhlGBRJQV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When your data has a lot of correlated features (e.g., predicting pizza sales based on different time slots), Elastic Net works best because it combines the strengths of both Lasso and Ridge.\n"
      ],
      "metadata": {
        "id": "TiOOUJeORLfT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Impact of the Regularization Parameter (λ) in Logistic Regression\n"
      ],
      "metadata": {
        "id": "VzHGNVcLRNLo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Effect:\n",
        "High λ leads to strong regularization (underfitting, too simple).\n",
        "\n",
        "\n",
        "Low λ allows more flexibility (overfitting, too complex).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WjYXGSwYROxm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Key Assumptions of Logistic Regression\n"
      ],
      "metadata": {
        "id": "4CoSEphuRRF8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mnemonics: “LINE”\n",
        "L: Linear relationship between features and log-odds.\n",
        "\n",
        "\n",
        "I: Independence of observations (no correlation between data points).\n",
        "\n",
        "\n",
        "N: No multicollinearity (predictors should not be highly correlated).\n",
        "\n",
        "\n",
        "E: Errors are random and independent.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aTIZIU50RV-l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.  What are some alternatives to Logistic Regression for classification tasks\n"
      ],
      "metadata": {
        "id": "xQE-WXzmRW3D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decision Trees: Works well for non-linear data.\n",
        "\n",
        "\n",
        "Random Forest: Ensemble method of decision trees.\n",
        "\n",
        "\n",
        "Support Vector Machines (SVM): Maximizes the margin between classes.\n",
        "\n",
        "\n",
        "Neural Networks: Powerful for complex, high-dimensional data.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HDmS6RuVRYr0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11.  What are Classification Evaluation Metric\n"
      ],
      "metadata": {
        "id": "0YZisU8mRn-j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imagine you're grading a test. These metrics tell you how well you did:\n",
        "Accuracy: Percentage of correct predictions.\n",
        "\n",
        "\n",
        "Precision: How many of the predicted positive outcomes were actually positive.\n",
        "\n",
        "\n",
        "Recall: How many of the actual positive outcomes were predicted as positive.\n",
        "\n",
        "\n",
        "F1-Score: Balance between precision and recall.\n",
        "\n",
        "\n",
        "ROC-AUC: Overall classification performance (how well the model discriminates between classes).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ru6d2IiLRqNT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. How does class imbalance affect Logistic Regression\n"
      ],
      "metadata": {
        "id": "cKQTET5HRq0P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Solution:\n",
        "Resampling techniques: Balance the dataset by over-sampling the minority class or under-sampling the majority class.\n",
        "\n",
        "\n",
        "Weighted loss functions: Adjust the cost function to penalize the model more for misclassifying the minority class.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6DXBcebMRuu9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13.What is Hyperparameter Tuning in Logistic Regression\n",
        "\n"
      ],
      "metadata": {
        "id": "7NzswjPJRxDO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analogy:\n",
        "Hyperparameter tuning is like adjusting oven temperature when baking a pizza. By changing certain settings (e.g., regularization strength, solver choice), you optimize the model’s performance."
      ],
      "metadata": {
        "id": "rOAgpy8YR2cr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. What are different solvers in Logistic Regression? Which one should be used  "
      ],
      "metadata": {
        "id": "XNRwJbAzR3a9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Analogy:\n",
        "Solvers are like different pizza-making methods:\n",
        "liblinear: Great for small datasets.\n",
        "\n",
        "\n",
        "lbfgs: Fast for large datasets and multiclass problems.\n",
        "\n",
        "\n",
        "saga: Suitable for large and sparse datasets.\n"
      ],
      "metadata": {
        "id": "CZqJtD10R8Yd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15.  How is Logistic Regression extended for multiclass classification  \n",
        "Methods:\n"
      ],
      "metadata": {
        "id": "91AYG5IRR84s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "One-vs-Rest (OvR): Builds one binary classifier for each class and selects the one with the highest probability.\n",
        "\n",
        "\n",
        "Softmax: Generalizes Logistic Regression for multiclass classification, assigning probabilities across multiple classes."
      ],
      "metadata": {
        "id": "wri8CziXSEI9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16.  What are the advantages and disadvantages of Logistic Regression\n",
        "Advantages:"
      ],
      "metadata": {
        "id": "sCF614dDSEv9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Simple and interpretable.\n",
        "\n",
        "\n",
        "Efficient for small datasets.\n",
        "\n",
        "\n",
        "Works well when the relationship between the predictors and outcome is linear.\n",
        "\n",
        "\n",
        "Disadvantages:\n",
        "Assumes a linear relationship between the log-odds and predictors.\n",
        "\n",
        "\n",
        "Struggles with non-linear relationships.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "afzy92ifSKEE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17.  What are some use cases of Logistic Regression\n",
        "\n"
      ],
      "metadata": {
        "id": "mHb2RipiSNgs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Examples:\n",
        "Medical diagnosis: Predicting whether a patient has a disease or not.\n",
        "\n",
        "\n",
        "Customer churn prediction: Predicting whether a customer will leave a service.\n",
        "\n",
        "\n",
        "Fraud detection: Classifying whether a transaction is fraudulent."
      ],
      "metadata": {
        "id": "g2NCOYGDSQQj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. What is the difference between Softmax Regression and Logistic Regression\n",
        "\n"
      ],
      "metadata": {
        "id": "muIlQXHlSQ5z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Key Difference:\n",
        "Logistic Regression is used for binary classification.\n",
        "\n",
        "\n",
        "Softmax is used for multiclass classification."
      ],
      "metadata": {
        "id": "DXa04JrySV2k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification  "
      ],
      "metadata": {
        "id": "kz9bLbyoSWUk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "When to Choose:\n",
        "One-vs-Rest is easier to implement and interpret for simpler multiclass problems.\n",
        "\n",
        "\n",
        "Softmax is better when classes are highly interdependent.\n"
      ],
      "metadata": {
        "id": "UATEHVsVSa31"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20.  How do we interpret coefficients in Logistic Regression?\n"
      ],
      "metadata": {
        "id": "BzPT-OTnScI8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the coefficient of a predictor is 0.7, increasing that predictor by 1 unit increases the log-odds of the outcome by 0.7.\n",
        "Simple Rule:\n",
        "Exponentiate coefficients to interpret them as odds ratios:\n",
        " ecoefficiente^{\\text{coefficient}}ecoefficient\n"
      ],
      "metadata": {
        "id": "b9TJ0DQYSm8D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "practical"
      ],
      "metadata": {
        "id": "fZEAymT5Sp8k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1 Write a Python program that loads a dataset, splits it into training and testing sets, applies Logistic Regression, and prints the model accuracy\n"
      ],
      "metadata": {
        "id": "V54mDB67Svw8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load a sample dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate accuracy\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Model Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I1iXvlx1S5K1",
        "outputId": "3c10b460-d5f5-46c6-c929-c9c0f6f9a0d7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Write a Python program to apply L1 regularization (Lasso) on a dataset using LogisticRegression(penalty='l1') and print the model accuracy\n"
      ],
      "metadata": {
        "id": "mGvChuAxTAcg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load and split data\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Logistic Regression with L1 regularization (Lasso)\n",
        "model = LogisticRegression(penalty='l1', solver='liblinear', max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate accuracy\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Model Accuracy with L1 Regularization (Lasso):\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nHZXwzHfTCqm",
        "outputId": "aece0237-2160-4f12-a1f4-d79144a3351d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy with L1 Regularization (Lasso): 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Write a Python program to train Logistic Regression with L2 regularization (Ridge) using LogisticRegression(penalty='l2'). Print model accuracy and coefficients\n"
      ],
      "metadata": {
        "id": "HIWXjUfSTE7z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load and split data\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Logistic Regression with L2 regularization (Ridge)\n",
        "model = LogisticRegression(penalty='l2', solver='liblinear', max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate accuracy\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Model Accuracy with L2 Regularization (Ridge):\", accuracy_score(y_test, y_pred))\n",
        "\n",
        "# Print coefficients\n",
        "print(\"Coefficients:\", model.coef_)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-SxOvn8UTHSc",
        "outputId": "3ac9175a-7b16-4d8b-833a-f92585801a11"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy with L2 Regularization (Ridge): 0.9777777777777777\n",
            "Coefficients: [[ 0.36479402  1.35499766 -2.09628559 -0.92154751]\n",
            " [ 0.4808915  -1.58463288  0.3937527  -1.09224057]\n",
            " [-1.5286415  -1.43244729  2.3048277   2.08584535]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4 Write a Python program to train Logistic Regression with Elastic Net Regularization (penalty='elasticnet')\n"
      ],
      "metadata": {
        "id": "WNQJK8B5TJ60"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load and split data\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Logistic Regression with ElasticNet regularization\n",
        "model = LogisticRegression(penalty='elasticnet', solver='saga', max_iter=200, l1_ratio=0.5)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate accuracy\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Model Accuracy with Elastic Net Regularization:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6M2Flpi2TL10",
        "outputId": "9adab4e1-0048-4a4f-c632-af05397e8aa7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy with Elastic Net Regularization: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5 Write a Python program to train a Logistic Regression model for multiclass classification using multi_class='ovr'\n"
      ],
      "metadata": {
        "id": "br7CQFbyTOYq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load and split data\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Logistic Regression with One-vs-Rest (OvR) for multiclass\n",
        "model = LogisticRegression(multi_class='ovr', solver='liblinear', max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate accuracy\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Multiclass Logistic Regression Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uG2JEot0TQUt",
        "outputId": "ab1b6db8-06fd-4ab0-99cd-a86dd790507f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Multiclass Logistic Regression Accuracy: 0.9777777777777777\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6 Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic Regression. Print the best parameters and accurac\n"
      ],
      "metadata": {
        "id": "lNjkXkkOTSae"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load and split data\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Hyperparameter grid for tuning\n",
        "param_grid = {'C': [0.1, 1, 10], 'penalty': ['l1', 'l2']}\n",
        "\n",
        "# GridSearchCV\n",
        "grid_search = GridSearchCV(LogisticRegression(solver='liblinear', max_iter=200), param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters and accuracy\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Best Accuracy:\", grid_search.best_score_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HIt3HusYTUi8",
        "outputId": "10b705d1-c481-44ab-a65f-ab7e5a4ff646"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'C': 10, 'penalty': 'l2'}\n",
            "Best Accuracy: 0.9523809523809523\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Write a Python program to evaluate Logistic Regression using Stratified K-Fold Cross-Validation. Print the average accuracy\n"
      ],
      "metadata": {
        "id": "UsKuIfy8TWfr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load and split data\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Stratified K-Fold Cross-Validation\n",
        "skf = StratifiedKFold(n_splits=5)\n",
        "model = LogisticRegression(max_iter=200)\n",
        "\n",
        "accuracies = []\n",
        "for train_index, test_index in skf.split(X, y):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracies.append(accuracy_score(y_test, y_pred))\n",
        "\n",
        "print(\"Average Accuracy using Stratified K-Fold Cross-Validation:\", sum(accuracies) / len(accuracies))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5MqGEYq5TYkZ",
        "outputId": "2cf211f5-e71b-4342-cb8a-4c1d72196d0a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Accuracy using Stratified K-Fold Cross-Validation: 0.9733333333333334\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its accuracy\n"
      ],
      "metadata": {
        "id": "Vo0b5HH6Tg0r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset from CSV\n",
        "df = pd.read_csv('your_dataset.csv')\n",
        "\n",
        "# Assuming 'target' is the column you want to predict\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate accuracy\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "id": "0l2S2O0lThb-",
        "outputId": "add3ab5f-5b32-4178-874e-c486dbe7c30b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'your_dataset.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-74732311cc12>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Load dataset from CSV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'your_dataset.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Assuming 'target' is the column you want to predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'your_dataset.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Write a Python program to apply RandomizedSearchCV for tuning hyperparameters (C, penalty, solver) in Logistic Regression. Print the best parameters and accuracy\n"
      ],
      "metadata": {
        "id": "EeK107UATkrk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load and split data\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Hyperparameter grid for RandomizedSearchCV\n",
        "param_dist = {'C': [0.1, 1, 10], 'penalty': ['l1', 'l2'], 'solver': ['liblinear']}\n",
        "\n",
        "# RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(LogisticRegression(max_iter=200), param_dist, n_iter=5, cv=5)\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters and accuracy\n",
        "print(\"Best Parameters:\", random_search.best_params_)\n",
        "print(\"Best Accuracy:\", random_search.best_score_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wyn5Jn_GTocG",
        "outputId": "e5bbe14d-d84a-42ba-916b-4ed6a721d74c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'solver': 'liblinear', 'penalty': 'l1', 'C': 1}\n",
            "Best Accuracy: 0.9428571428571428\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Write a Python program to implement One-vs-One (OvO) Multiclass Logistic Regression and print accuracy\n"
      ],
      "metadata": {
        "id": "VEVFIJUkTrvU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load and split data\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Logistic Regression with One-vs-One (OvO) for multiclass\n",
        "try:\n",
        "    model = LogisticRegression(multi_class='ovo', solver='liblinear', max_iter=200)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Predict and evaluate accuracy\n",
        "    y_pred = model.predict(X_test)\n",
        "    print(\"One-vs-One Multiclass Logistic Regression Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "except Exception as e:\n",
        "    print(\"An error occurred:\", e)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uiVnLuTyTxUT",
        "outputId": "eda9f9ad-1674-4e72-955b-fa97f1ab78ee"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An error occurred: The 'multi_class' parameter of LogisticRegression must be a str among {'ovr', 'multinomial', 'auto'}. Got 'ovo' instead.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "11 Write a Python program to train a Logistic Regression model and visualize the confusion matrix for binary classification\n"
      ],
      "metadata": {
        "id": "MAEpgTMKTz1v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=10000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=data.target_names)\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "tAn436o0UArj",
        "outputId": "4d7f0e64-83a7-461e-bddf-ef1d7b92c1f0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "12 Write a Python program to train a Logistic Regression model and evaluate its performance using Precision, Recall, and F1-Score\n"
      ],
      "metadata": {
        "id": "JMAOMXh-UCx7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Evaluate precision, recall, and f1-score\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1-Score: {f1}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nXp-l61GUDSH",
        "outputId": "7e74c1a0-dac9-4b78-f34c-19c036deae6a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.9814814814814815\n",
            "Recall: 0.9814814814814815\n",
            "F1-Score: 0.9814814814814815\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "13.  Write a Python program to train a Logistic Regression model on imbalanced data and apply class weights to improve model performance"
      ],
      "metadata": {
        "id": "9bMGtGE2UFJq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Create imbalanced dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=2, n_redundant=10, weights=[0.9, 0.1], random_state=42)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Logistic Regression with class weights\n",
        "model = LogisticRegression(class_weight='balanced')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNnnfIfBUHEy",
        "outputId": "a3947fdb-8f5f-4813-d82e-851d34c95cb8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8766666666666667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. Write a Python program to train Logistic Regression on the Titanic dataset, handle missing values, and evaluate performance\n"
      ],
      "metadata": {
        "id": "zviAWYtuUJG3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Titanic dataset\n",
        "df = pd.read_csv('titanic.csv')\n",
        "\n",
        "# Handle missing values (fill with median or drop)\n",
        "df['Age'].fillna(df['Age'].median(), inplace=True)\n",
        "df.dropna(subset=['Embarked'], inplace=True)\n",
        "\n",
        "# Select features and target\n",
        "X = df[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare']]\n",
        "X = pd.get_dummies(X, drop_first=True)  # Convert categorical features\n",
        "y = df['Survived']\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Logistic Regression\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "id": "4DrAV6QFUL1K",
        "outputId": "b256d069-a618-41f9-80cb-1474cc5a6035"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'titanic.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-8d57aef609f0>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Load Titanic dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'titanic.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Handle missing values (fill with median or drop)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'titanic.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "15 Write a Python program to apply feature scaling (Standardization) before training a Logistic Regression model. Evaluate its accuracy and compare results with and without scaling\n"
      ],
      "metadata": {
        "id": "DzGV4EnfUOk6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Feature scaling (Standardization)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression\n",
        "model = LogisticRegression(max_iter=10000)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Evaluate accuracy\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "print(f\"Accuracy with Scaling: {accuracy_score(y_test, y_pred)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nt9NKe6bUQgT",
        "outputId": "661b8fbe-adb7-435e-90a9-745c61733a07"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with Scaling: 0.9824561403508771\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "16.  Write a Python program to train Logistic Regression and evaluate its performance using ROC-AUC score\n"
      ],
      "metadata": {
        "id": "U6FiLZPbUSzE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load dataset\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Logistic Regression\n",
        "model = LogisticRegression(max_iter=10000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities\n",
        "y_probs = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Evaluate ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_probs)\n",
        "print(f\"ROC-AUC Score: {roc_auc}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDfOHEcoUUjj",
        "outputId": "f7d8de27-431e-4ae0-9e85-ceae897df9e4"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC Score: 0.9976484420928865\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "17.  Write a Python program to train Logistic Regression using a custom learning rate (C=0.5) and evaluate accuracy\n"
      ],
      "metadata": {
        "id": "mN4vsYAlUWlH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Logistic Regression with custom learning rate (C=0.5)\n",
        "model = LogisticRegression(C=0.5, max_iter=10000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate accuracy\n",
        "y_pred = model.predict(X_test)\n",
        "print(f\"Accuracy with Custom Learning Rate: {accuracy_score(y_test, y_pred)}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BsX0zDiLUYaS",
        "outputId": "301c2099-7c31-40bb-e2a9-4f16a1ca1e87"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with Custom Learning Rate: 0.9766081871345029\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. Write a Python program to train Logistic Regression and identify important features based on model coefficients\n"
      ],
      "metadata": {
        "id": "263aO5UUUa6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load dataset\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Logistic Regression\n",
        "model = LogisticRegression(max_iter=10000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Get important features (coefficients)\n",
        "coefficients = model.coef_[0]\n",
        "feature_importance = sorted(zip(data.feature_names, coefficients), key=lambda x: abs(x[1]), reverse=True)\n",
        "\n",
        "print(\"Important Features based on Coefficients:\")\n",
        "for feature, coef in feature_importance:\n",
        "    print(f\"{feature}: {coef}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2w_QMyxnUcsY",
        "outputId": "10f8ab04-9dd4-4cc3-cafd-f120cf9b0d30"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Important Features based on Coefficients:\n",
            "texture error: 1.3615073190284872\n",
            "worst concavity: -1.3371979815250603\n",
            "mean radius: 1.0467945718592868\n",
            "worst compactness: -0.7152413312598583\n",
            "worst symmetry: -0.7119436737980998\n",
            "mean concavity: -0.5120053512568038\n",
            "worst texture: -0.5054544486706379\n",
            "worst concave points: -0.4945826193252438\n",
            "perimeter error: 0.44821504218879415\n",
            "mean perimeter: -0.38602860481784607\n",
            "worst smoothness: -0.274620759651435\n",
            "mean concave points: -0.2738590516162102\n",
            "mean compactness: -0.23613108799129856\n",
            "mean texture: 0.2319967523697633\n",
            "mean symmetry: -0.21861492232098087\n",
            "area error: -0.14499775336637316\n",
            "mean smoothness: -0.13653868452221726\n",
            "radius error: -0.11916662133603316\n",
            "worst fractal dimension: -0.09935961075847118\n",
            "concavity error: -0.06654548594029253\n",
            "worst perimeter: -0.060612118273019336\n",
            "symmetry error: -0.04411917131362737\n",
            "worst radius: 0.041321752784449814\n",
            "mean fractal dimension: -0.038264102280015026\n",
            "concave points error: -0.035049938294170545\n",
            "mean area: 0.025831021017294283\n",
            "smoothness error: -0.018328974369941093\n",
            "worst area: -0.010931337258030993\n",
            "compactness error: 0.007056019606347905\n",
            "fractal dimension error: 0.0007119590774372576\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. Write a Python program to train Logistic Regression and evaluate its performance using Cohen’s Kappa Score"
      ],
      "metadata": {
        "id": "pUQPDNlMUfZb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import cohen_kappa_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load dataset\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Logistic Regression\n",
        "model = LogisticRegression(max_iter=10000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate Cohen's Kappa\n",
        "kappa = cohen_kappa_score(y_test, y_pred)\n",
        "print(f\"Cohen's Kappa Score: {kappa}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gEPAHqs6UhJs",
        "outputId": "8c181de4-b5e1-4434-d78f-57e1c01391af"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cohen's Kappa Score: 0.9497354497354498\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "20 Write a Python program to train Logistic Regression and visualize the Precision-Recall Curve for binary classification"
      ],
      "metadata": {
        "id": "gS7q-50GUj6a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=10000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities\n",
        "y_probs = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Get Precision-Recall curve values\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_probs)\n",
        "\n",
        "# Plot the Precision-Recall curve\n",
        "plt.plot(recall, precision, color='blue')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "OV9v-JO0Umsx",
        "outputId": "dbca14f2-6623-4e63-b1aa-417f7e672f82"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "21 Write a Python program to train Logistic Regression with different solvers (liblinear, saga, lbfgs) and compare their accuracy\n"
      ],
      "metadata": {
        "id": "gpO8pHfoUpYZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# List of solvers to test\n",
        "solvers = ['liblinear', 'saga', 'lbfgs']\n",
        "for solver in solvers:\n",
        "    # Train Logistic Regression with the current solver\n",
        "    model = LogisticRegression(solver=solver, max_iter=10000)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Predict and evaluate accuracy\n",
        "    y_pred = model.predict(X_test)\n",
        "    print(f\"Accuracy using {solver} solver: {accuracy_score(y_test, y_pred)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOxlreJlUrh0",
        "outputId": "7dfe015b-7c6a-4ad8-bc6b-2d1c7553ea18"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy using liblinear solver: 0.9649122807017544\n",
            "Accuracy using saga solver: 0.9649122807017544\n",
            "Accuracy using lbfgs solver: 0.9766081871345029\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. Write a Python program to train Logistic Regression and evaluate its performance using Matthews Correlation Coefficient (MCC)\n"
      ],
      "metadata": {
        "id": "cR95j6PTUwOq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=10000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate MCC\n",
        "mcc = matthews_corrcoef(y_test, y_pred)\n",
        "print(f\"Matthews Correlation Coefficient (MCC): {mcc}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8sfILhaLUwub",
        "outputId": "60905683-9b09-45f3-d9cd-baefe34df01a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matthews Correlation Coefficient (MCC): 0.9497354497354498\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "23.  Write a Python program to train Logistic Regression on both raw and standardized data. Compare their accuracy to see the impact of feature scaling\n"
      ],
      "metadata": {
        "id": "ykYpfIvFUzXJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Logistic Regression on raw data\n",
        "model_raw = LogisticRegression(max_iter=10000)\n",
        "model_raw.fit(X_train, y_train)\n",
        "y_pred_raw = model_raw.predict(X_test)\n",
        "accuracy_raw = accuracy_score(y_test, y_pred_raw)\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression on standardized data\n",
        "model_scaled = LogisticRegression(max_iter=10000)\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "print(f\"Accuracy on Raw Data: {accuracy_raw}\")\n",
        "print(f\"Accuracy on Scaled Data: {accuracy_scaled}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TUnMNwVSU0_j",
        "outputId": "6543fec9-c6ce-4c91-9148-8391d938a4db"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on Raw Data: 0.9766081871345029\n",
            "Accuracy on Scaled Data: 0.9824561403508771\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "24  Write a Python program to train Logistic Regression and find the optimal C (regularization strength) using cross-validation\n"
      ],
      "metadata": {
        "id": "WoG_2dSOU3Oi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Set up Logistic Regression and hyperparameter grid\n",
        "model = LogisticRegression(max_iter=10000)\n",
        "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
        "\n",
        "# Perform GridSearchCV to find the best C\n",
        "grid_search = GridSearchCV(model, param_grid, cv=5)\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "# Print best parameter and score\n",
        "print(f\"Best C: {grid_search.best_params_['C']}\")\n",
        "print(f\"Best cross-validation score: {grid_search.best_score_}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-XJ5o4aU4xK",
        "outputId": "191677a6-4b95-4ce5-ef23-9f018f6c2f1e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best C: 100\n",
            "Best cross-validation score: 0.9613569321533924\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "25  Write a Python program to train Logistic Regression, save the trained model using joblib, and load it again to make predictions\n"
      ],
      "metadata": {
        "id": "E3vz5gD4U8zs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=10000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Save the trained model\n",
        "joblib.dump(model, 'logistic_regression_model.joblib')\n",
        "\n",
        "# Load the model from file\n",
        "loaded_model = joblib.load('logistic_regression_model.joblib')\n",
        "\n",
        "# Make predictions with the loaded model\n",
        "y_pred = loaded_model.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "print(f\"Accuracy from loaded model: {accuracy_score(y_test, y_pred)}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NnukCLkyU9kq",
        "outputId": "432f4124-566d-4714-a7d2-84991829e1a1"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy from loaded model: 0.9766081871345029\n"
          ]
        }
      ]
    }
  ]
}